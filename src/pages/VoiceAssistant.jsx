import { useNavigate } from '@solidjs/router';
import { createSignal, Show, onCleanup } from 'solid-js';
import { createEvent } from '../supabaseClient';
import { createNotification } from '../components/Notification';
import { SolidMarkdown } from 'solid-markdown';
import Loader from '../components/Loader';

function VoiceAssistant() {
  const navigate = useNavigate();

  const [listening, setListening] = createSignal(false);
  const [loading, setLoading] = createSignal(false);
  const [transcript, setTranscript] = createSignal('');
  const [assistantResponse, setAssistantResponse] = createSignal('');
  const [loadingAudio, setLoadingAudio] = createSignal(false);

  const { NotificationComponent, showNotification } = createNotification();
  let currentAudio = null;

  let recognition;

  const startListening = () => {
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      alert('Ù…ØªØµÙØ­Ùƒ Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª.');
      return;
    }

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

    recognition = new SpeechRecognition();
    recognition.lang = 'ar-EG';
    recognition.continuous = false;

    recognition.onstart = () => {
      setListening(true);
    };

    recognition.onend = () => {
      setListening(false);
    };

    recognition.onresult = (event) => {
      const speechResult = event.results[0][0].transcript;
      setTranscript(speechResult);
      handleAssistantRequest(speechResult);
    };

    recognition.onerror = (event) => {
      console.error('Error during recognition:', event.error);
      setListening(false);
    };

    recognition.start();
  };

  const handleAssistantRequest = async (inputText) => {
    setLoading(true);
    try {
      const result = await createEvent('chatgpt_request', {
        prompt: inputText,
        response_type: 'text',
      });
      setAssistantResponse(result || 'Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯.');
      handleSpeakResponse(result || 'Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯.');
    } catch (error) {
      console.error('Error:', error);
      showNotification('Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯.', 'error');
    } finally {
      setLoading(false);
    }
  };

  const handleCopyResponse = () => {
    if (assistantResponse()) {
      navigator.clipboard
        .writeText(assistantResponse())
        .then(() => {
          showNotification('ØªÙ… Ù†Ø³Ø® Ø§Ù„Ø±Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø§ÙØ¸Ø©', 'success');
        })
        .catch((error) => {
          console.error('ÙØ´Ù„ Ø§Ù„Ù†Ø³Ø®:', error);
          showNotification('ÙØ´Ù„ Ù†Ø³Ø® Ø§Ù„Ø±Ø¯', 'error');
        });
    }
  };

  const handleSpeakResponse = async (text) => {
    if (currentAudio) {
      currentAudio.pause();
      currentAudio = null;
    }
    setLoadingAudio(true);
    try {
      const audioUrl = await createEvent('text_to_speech', {
        text: text,
      });
      currentAudio = new Audio(audioUrl);
      currentAudio.play();
    } catch (error) {
      console.error('Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù…:', error);
      showNotification('Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª.', 'error');
    } finally {
      setLoadingAudio(false);
    }
  };

  onCleanup(() => {
    if (currentAudio) {
      currentAudio.pause();
      currentAudio = null;
    }
  });

  return (
    <div class="flex flex-col items-center p-4 h-full text-gray-800 pt-8 pb-16">
      <NotificationComponent />
      <button
        onClick={() => navigate(-1)}
        class="self-start mb-4 text-2xl cursor-pointer"
      >
        ðŸ”™
      </button>
      <h1 class="text-4xl font-bold text-purple-600 mb-6">Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ØµÙˆØªÙŠ</h1>

      <button
        onClick={startListening}
        class={`px-6 py-3 bg-blue-500 text-white rounded-lg hover:bg-blue-600 transition duration-300 ease-in-out transform hover:scale-105 ${
          listening() || loading() ? 'opacity-50 cursor-not-allowed' : 'cursor-pointer'
        }`}
        disabled={listening() || loading()}
      >
        <Show when={!listening()} fallback="Ø§Ø³ØªÙ…Ø¹...">
          <Show when={!loading()} fallback="Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...">
            Ø§Ø¶ØºØ· Ù„Ù„ØªØ­Ø¯Ø«
          </Show>
        </Show>
      </button>

      <Show when={transcript()}>
        <div class="w-full max-w-md mt-6 p-4 bg-white rounded-lg shadow-md">
          <h3 class="text-xl font-bold mb-2 text-purple-600">Ù†ØµÙƒ:</h3>
          <p class="text-gray-700 whitespace-pre-wrap mb-4">{transcript()}</p>
        </div>
      </Show>

      <Show when={assistantResponse()}>
        <div class="w-full max-w-md mt-6 p-4 bg-white rounded-lg shadow-md">
          <h3 class="text-xl font-bold mb-2 text-purple-600">Ø±Ø¯ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯:</h3>
          <SolidMarkdown class="prose prose-lg text-gray-700 mb-4" children={assistantResponse()} />
          <button
            onClick={handleCopyResponse}
            class="w-full px-4 py-2 bg-green-500 text-white rounded-lg hover:bg-green-600 transition duration-300 ease-in-out transform hover:scale-105 cursor-pointer"
          >
            Ù†Ø³Ø®
          </button>
          <button
            onClick={() => handleSpeakResponse(assistantResponse())}
            class={`w-full mt-2 px-4 py-2 bg-yellow-500 text-white rounded-lg hover:bg-yellow-600 transition duration-300 ease-in-out transform hover:scale-105 ${
              loadingAudio() ? 'opacity-50 cursor-not-allowed' : 'cursor-pointer'
            }`}
            disabled={loadingAudio()}
          >
            <Show when={!loadingAudio()} fallback={<Loader loading={loadingAudio()} />}>
              Ø§Ø³ØªÙ…Ø§Ø¹
            </Show>
          </button>
        </div>
      </Show>
    </div>
  );
}

export default VoiceAssistant;